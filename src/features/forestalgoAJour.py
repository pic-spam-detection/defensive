# -*- coding: utf-8 -*-
"""ForestAlgo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_N5_i6-UMT5SFgt1OafOGQWw09cP30QD

**Forest Model**

**Chargement du dataset**
"""

import urllib.request
import zipfile
import os
import pandas as pd


def download_dataset():
    if not os.path.exists("./data"):
        os.makedirs("data")

    if not os.path.exists("./data/enron_spam_data.csv"):
        if not os.path.exists("./data/dataset_enron_annoted.zip"):
            print("Téléchargement du dataset...")
            try:
                urllib.request.urlretrieve(
                    "https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip",
                    "./data/dataset_enron_annoted.zip",
                )
                print("Téléchargement réussi.")
            except:
                print("Téléchargement échoué")
                exit(1)

        if not os.path.exists("./data/enron_spam_data.csv"):
            print("Extraction du dataset")
            with zipfile.ZipFile("./data/dataset_enron_annoted.zip", "r") as zip_ref:
                zip_ref.extractall("./data/")
            print("Extraction réussie.")

    return "./data/enron_spam_data.csv"


def load_dataset():
    download_dataset()
    return pd.read_csv("./data/enron_spam_data.csv")


def get_dataset():
    dataset = load_dataset()
    len_dataset = len(dataset)
    dataset = dataset.drop(columns=["Date"])
    dataset["adress"] = ""
    dataset["domain"] = ""
    dataset["domain_extension"] = ""
    dataset.rename(columns={"Subject": "subject", "Message": "body"}, inplace=True)
    dataset["ground_truth"] = dataset["Spam/Ham"].apply(
        lambda x: 0 if x == "ham" else 1
    )
    dataset = dataset.drop(columns="Spam/Ham")

    dataset_train = dataset.iloc[: int(len_dataset * 0.8)]
    dataset_test = dataset.iloc[int(len_dataset * 0.8) :]

    dataset_train = dataset_train.sample(len(dataset_train), random_state=42)
    dataset_test = dataset_test.sample(len(dataset_test), random_state=42)

    dataset = {"train": dataset_train, "test": dataset_test}

    return dataset


import pandas as pd
import re
from collections import Counter

# Charger le dataset
data = get_dataset()

# Récupérer les DataFrames d'entraînement et de test
train_df = data["train"]
test_df = data["test"]

# Vérifier les colonnes disponibles
print(f" Colonnes du dataset : {train_df.columns}")

# Définition des expressions régulières et listes de mots-clés
URL_PATTERN = re.compile(r"https?://\S+|www\.\S+")
SPAM_KEYWORDS = {"free", "win", "money", "prize", "buy now", "urgent"}


# Fonction d'extraction des features
def extraire_features(subject, body):
    mail = f"{subject} {body}"  # Fusionner sujet et corps du mail
    mots = re.findall(r"\b\w+\b", mail.lower())
    mots_freq = Counter(mots)
    nombre_majuscules = sum(1 for mot in mots if mot.isupper())
    nombre_mots = len(mots)
    nombre_caracteres = len(mail)
    densite_majuscules = nombre_majuscules / nombre_mots if nombre_mots > 0 else 0
    densite_exclamations = mail.count("!") / nombre_mots if nombre_mots > 0 else 0
    densite_urls = (
        len(URL_PATTERN.findall(mail)) / nombre_mots if nombre_mots > 0 else 0
    )

    result = {
        "spam_mots_clefs": any(word in mots_freq for word in SPAM_KEYWORDS),
        "contient_url": bool(URL_PATTERN.search(mail)),
        "trop_exclamations": mail.count("!") > 5,
        "trop_majuscules": nombre_majuscules > nombre_mots * 0.3,
        "longueur_suspecte": nombre_caracteres > 5000,
        "nombre_mots": nombre_mots,
        "nombre_caracteres": nombre_caracteres,
        "densite_majuscules": densite_majuscules,
        "densite_exclamations": densite_exclamations,
        "densite_urls": densite_urls,
    }

    return result


import pandas as pd

# Appliquer la fonction d'extraction des features
train_features = train_df.apply(
    lambda row: extraire_features(row["subject"], row["body"]), axis=1
)
test_features = test_df.apply(
    lambda row: extraire_features(row["subject"], row["body"]), axis=1
)

# Convertir en DataFrame et ajouter les labels
train_features_df = pd.DataFrame(list(train_features))
train_features_df["label"] = train_df["ground_truth"]

test_features_df = pd.DataFrame(list(test_features))
test_features_df["label"] = test_df["ground_truth"]

# Vérifier les données
print(train_features_df.head())

"""**Random Forest Model**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
import numpy as np

# Vérifier la distribution initiale des classes
print(" Répartition des classes AVANT correction:")
print(train_features_df["label"].value_counts())

# Appliquer la stratification pour s'assurer que `y_test` contient des spam
X_train, X_test, y_train, y_test = train_test_split(
    train_features_df.drop(columns=["label"]),
    train_features_df["label"],
    test_size=0.2,
    random_state=42,
    stratify=train_features_df[
        "label"
    ],  #  Assure une répartition équitable des classes
)

# Vérifier si `y_test` contient bien des spams
print("\n Répartition des classes DANS y_test après stratification:")
print(y_test.value_counts())

# Vérifier si `y_train` est fortement déséquilibré
if y_train.value_counts()[1] < 10:  # Moins de 10 spams ? C'est un problème !
    print("\n Problème: Trop peu de spams dans y_train ! On applique SMOTE.")

    # Appliquer SMOTE pour équilibrer les classes
    smote = SMOTE(random_state=42)
    X_train, y_train = smote.fit_resample(X_train, y_train)

    print(" Répartition des classes DANS y_train APRÈS SMOTE:")
    print(y_train.value_counts())

# Vérifier la taille finale des datasets
print(f"\n Taille X_train : {X_train.shape}, y_train : {y_train.shape}")
print(f" Taille X_test : {X_test.shape}, y_test : {y_test.shape}")

# Entraîner le modèle Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Prédictions
y_pred = model.predict(X_test)

# Vérifier si y_pred contient des NaN
if np.isnan(y_pred).sum() > 0:
    print(" Problème : Les prédictions contiennent des NaN !")

# Évaluer le modèle
print(f"\n Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))
