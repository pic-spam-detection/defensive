# -*- coding: utf-8 -*-
"""Features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WU2BPfAmwwCSX4caoDi1BqrsjsvgNPLn

**Chargement du dataset**
"""

import urllib.request
import zipfile
import os
import pandas as pd

def download_dataset():
    if not os.path.exists("./data"):
        os.makedirs("data")

    if not os.path.exists("./data/enron_spam_data.csv"):
        if not os.path.exists("./data/dataset_enron_annoted.zip"):
            print("Téléchargement du dataset...")
            try:
                urllib.request.urlretrieve("https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip",
                                "./data/dataset_enron_annoted.zip")
                print("Téléchargement réussi.")
            except:
                print("Téléchargement échoué")
                exit(1)

        if not os.path.exists("./data/enron_spam_data.csv"):
            print("Extraction du dataset")
            with zipfile.ZipFile("./data/dataset_enron_annoted.zip", 'r') as zip_ref:
                zip_ref.extractall("./data/")
            print("Extraction réussie.")

    return "./data/enron_spam_data.csv"

def load_dataset():
    download_dataset()
    return pd.read_csv("./data/enron_spam_data.csv")

def get_dataset():
    dataset = load_dataset()
    len_dataset = len(dataset)
    dataset = dataset.drop(columns=["Date"])
    dataset["adress"] = ""
    dataset["domain"] = ""
    dataset["domain_extension"] = ""
    dataset.rename(columns={"Subject": "subject", "Message": "body"}, inplace=True)
    dataset["ground_truth"] = dataset["Spam/Ham"].apply(lambda x: 0 if x == "ham" else 1)
    dataset = dataset.drop(columns="Spam/Ham")

    dataset_train = dataset.iloc[:int(len_dataset * 0.8)]
    dataset_test = dataset.iloc[int(len_dataset * 0.8):]

    dataset_train = dataset_train.sample(len(dataset_train), random_state=42)
    dataset_test = dataset_test.sample(len(dataset_test), random_state=42)

    dataset = {"train": dataset_train, "test": dataset_test}

    return dataset

# Charger le dataset
data = get_dataset()

# Récupérer les DataFrames d'entraînement et de test
train_df = data["train"]
test_df = data["test"]

# Vérifier les colonnes disponibles
print(f" Colonnes du dataset : {train_df.columns}")

# Aperçu des premières lignes
print(train_df.head())

"""**Extraction des features et évaluation**"""

import re
import pandas as pd
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer

# Définition des expressions régulières et listes de mots-clés
URL_PATTERN = re.compile(r'https?://\S+|www\.\S+')
SPAM_KEYWORDS = {"free", "win", "money", "prize", "buy now", "urgent"}

# Fonction d'extraction des features
def extraire_features(subject, body):
    mail = f"{subject} {body}"  # Fusionner sujet et corps du mail
    mots = re.findall(r'\b\w+\b', mail.lower())
    mots_freq = Counter(mots)
    nombre_majuscules = sum(1 for mot in mots if mot.isupper())
    nombre_mots = len(mots)
    nombre_caracteres = len(mail)
    densite_majuscules = nombre_majuscules / nombre_mots if nombre_mots > 0 else 0
    densite_exclamations = mail.count('!') / nombre_mots if nombre_mots > 0 else 0
    densite_urls = len(URL_PATTERN.findall(mail)) / nombre_mots if nombre_mots > 0 else 0

    result = {
        "spam_mots_clefs": any(word in mots_freq for word in SPAM_KEYWORDS),
        "contient_url": bool(URL_PATTERN.search(mail)),
        "trop_exclamations": mail.count('!') > 5,
        "trop_majuscules": nombre_majuscules > nombre_mots * 0.3,
        "longueur_suspecte": nombre_caracteres > 5000,
        "nombre_mots": nombre_mots,
        "nombre_caracteres": nombre_caracteres,
        "densite_majuscules": densite_majuscules,
        "densite_exclamations": densite_exclamations,
        "densite_urls": densite_urls
    }

    return result

# Fonction d'évaluation si un email est spam ou pas
def evaluer_spam(features):
    score = 0

    # Points attribués pour chaque feature suspecte
    if features["spam_mots_clefs"]:
        score += 2
    if features["contient_url"]:
        score += 1
    if features["trop_exclamations"]:
        score += 1
    if features["trop_majuscules"]:
        score += 1
    if features["longueur_suspecte"]:
        score += 2
    if features["densite_majuscules"] > 0.2:
        score += 1
    if features["densite_exclamations"] > 0.1:
        score += 1
    if features["densite_urls"] > 0.05:
        score += 1

    # Seuil de décision : spam si score ≥ 3
    return "SPAM" if score >= 3 else "NON SPAM"

# Sélectionner aléatoirement 5 emails
sample_emails = train_df.sample(5)

# Appliquer la fonction d'extraction des features
sample_features = sample_emails.apply(lambda row: extraire_features(row["subject"], row["body"]), axis=1)

# Convertir en DataFrame et ajouter les emails originaux
features_df = pd.DataFrame(list(sample_features))
features_df["email_complet"] = sample_emails["subject"] + " " + sample_emails["body"]  # Ajouter l'email complet
features_df["is_spam"] = features_df.apply(evaluer_spam, axis=1)  # Évaluer si c'est un spam

# Réorganiser les colonnes pour affichage
features_df = features_df[["email_complet", "is_spam"] + [col for col in features_df.columns if col not in ["email_complet", "is_spam"]]]
# Afficher les emails sélectionnés avant les features
print("\n Emails sélectionnés :\n")
for index, row in sample_emails.iterrows():
    print(f" Sujet : {row['subject']}")
    print(f" Corps : {row['body']}\n")
    print("-" * 80)

# Affichage des features après les emails
print("\n Emails & Spam Features:")
print(features_df.head())

"""**Utilisation de Vectorizer TF-IDF**"""

import pandas as pd
import re
import numpy as np
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from tqdm import tqdm

# Télécharger les stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Charger le dataset
data = get_dataset()
train_df, test_df = data["train"], data["test"]

# Vérifier les colonnes disponibles
print(f" Colonnes du dataset : {train_df.columns}")

# Définition des expressions régulières et mots-clés spam
URL_PATTERN = re.compile(r'https?://\S+|www\.\S+')
SPAM_KEYWORDS = {"free", "win", "money", "prize", "buy now", "urgent"}

# Initialisation du Vectorizer TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words="english")

def extraire_features(subject, body):
    """ Fonction unique pour extraire toutes les features d'un email """

    # Fusionner sujet et corps
    mail = f"{subject} {body}"

    # Tokenisation
    mots = re.findall(r'\b\w+\b', mail.lower())
    mots_freq = Counter(mots)

    # Longueur du mail
    nombre_mots = len(mots)
    nombre_caracteres = len(mail)

    # Calcul des densités
    nombre_majuscules = sum(1 for mot in mots if mot.isupper())
    densite_majuscules = nombre_majuscules / nombre_mots if nombre_mots > 0 else 0
    densite_exclamations = mail.count("!") / nombre_mots if nombre_mots > 0 else 0
    densite_urls = len(URL_PATTERN.findall(mail)) / nombre_mots if nombre_mots > 0 else 0

    # Calcul de la densité des mots-clés spam
    nombre_mots_spam = sum(1 for word in mots if word in SPAM_KEYWORDS)
    densite_spam_mots_clefs = nombre_mots_spam / nombre_mots if nombre_mots > 0 else 0

    # Feature TF-IDF (besoin d’un entraînement préalable)
    tfidf_score = np.mean(tfidf_vectorizer.fit_transform([mail]).toarray()) if nombre_mots > 0 else 0

    # Retourner toutes les features
    return {
        "spam_mots_clefs": densite_spam_mots_clefs,
        "contient_url": bool(URL_PATTERN.search(mail)),
        "trop_exclamations": mail.count('!') > 5,
        "trop_majuscules": nombre_majuscules > nombre_mots * 0.3,
        "longueur_suspecte": nombre_caracteres > 5000,
        "nombre_mots": nombre_mots,
        "nombre_caracteres": nombre_caracteres,
        "densite_majuscules": densite_majuscules,
        "densite_exclamations": densite_exclamations,
        "densite_urls": densite_urls,
        "tfidf_moyen": tfidf_score
    }

# Appliquer la fonction d'extraction des features sur l'ensemble du dataset
tqdm.pandas()  # Progress bar pour le suivi
train_features = train_df.progress_apply(lambda row: extraire_features(row["subject"], row["body"]), axis=1)

# Convertir en DataFrame et ajouter les labels
train_features_df = pd.DataFrame(list(train_features))
train_features_df["label"] = train_df["ground_truth"]

# Afficher les résultats
print(" Aperçu des features extraites :")
print(train_features_df.head())